- **Temporal Locality** - This concerns reusing the same data within a short period of time, to avoid costly memory fetching. If data is accessed once and isn't accessed for some time it will likely be evicted from CPU cache or Translation Look-aside Buffer.
- **Spatial Locality** - This concerns accessing memory locations that are close together (typically in virtual memory). This is why arrays can be so efficient. Non-contiguous or scattered accesses (linked list) prevent efficient use of cache lines and prefetching.
- **Non-Uniform Memory Access** - This is the idea that memory latency and bandwidth depend on which core/thread is accessing which memory region. Each CPU socket or memory controller has its own attached DRAM, so accessing local memory is faster than access remote memory. For example, multi-socket Xeons or multi-chip EPYCs. Therefore to improve performance, cores should ideally access memory local to their own socket or die.
- **Cache Coherence** - There are set of protocols and rules that enforce a consistent view of memory across cores. The issue is that even without logical sharing, memory can "bounce" between caches due to the protocols. This links to the following below.
- **False Sharing** - Two (or more) threads/processes are writing data to the same cache line. When one thread writes to the cache line the whole line is dirty and needs to be flushed by the writer thread and re-fetched before writing by the other threads.
- **Memory Alignment** - This is the idea that data should be align to their natural boundaries, that is a 4-byte integer should be aligned to address that is divisible by 4. Misaligned accesses can cause extra memory reads as data may split across cache lines. Most of the time though unless the developer explicitly allows it memory alignment is unlikely to occur.
- **Cache Line** - Is the smallest amount of data that the CPU cache can read from or write to main memory at once, typically 64 bytes.
- **Task Affinity** - This is the idea that a task in the form of either a thread or a process should run on the same core (or maybe set of cores). OS scheduling normally moves threads between cores to load balance. This also helps keep the tasks working set "warm" in the core's caches, which reduces cache invalidation. This overall helps to ensure the thread mostly uses local memory in NUMA systems.
- **Data Affinity** - Similar to tasks affinity this is the concept of placing data in memory close to cores that access it the most. This is mainly a NUMA system problem because we want to avoid high remote memory access latency and improve spatial and temporal locality where a thread repeatedly works with the same data.
- **Temporal Locality** - This concerns reusing the same data within a short period of time, to avoid costly memory fetching. If data is accessed once and isn't accessed for some time it will likely be evicted from CPU cache or Translation Look-aside Buffer.
- **Spatial Locality** - This concerns accessing memory locations that are close together (typically in virtual memory). This is why arrays can be so efficient. Non-contiguous or scattered accesses (linked list) prevent efficient use of cache lines and prefetching.
- **Non-Uniform Memory Access** - This is the idea that memory latency and bandwidth depend on which core/thread is accessing which memory region. Each CPU socket or memory controller has its own attached DRAM, so accessing local memory is faster than access remote memory. For example, multi-socket Xeons or multi-chip EPYCs. Therefore to improve performance, cores should ideally access memory local to their own socket or die.
- **Cache Coherence** - There are set of protocols and rules that enforce a consistent view of memory across cores. The issue is that even without logical sharing, memory can "bounce" between caches due to the protocols. This links to the following below.
- **False Sharing** - Two (or more) threads/processes are writing data to the same cache line. When one thread writes to the cache line the whole line is dirty and needs to be flushed by the writer thread and re-fetched before writing by the other threads.
- **Memory Alignment** - This is the idea that data should be align to their natural boundaries, that is a 4-byte integer should be aligned to address that is divisible by 4. Misaligned accesses can cause extra memory reads as data may split across cache lines. Most of the time though unless the developer explicitly allows it memory alignment is unlikely to occur.
- **Cache Line** - Is the smallest amount of data that the CPU cache can read from or write to main memory at once, typically 64 bytes.
- **Task Affinity** - This is the idea that a task in the form of either a thread or a process should run on the same core (or maybe set of cores). OS scheduling normally moves threads between cores to load balance. This also helps keep the tasks working set "warm" in the core's caches, which reduces cache invalidation. This overall helps to ensure the thread mostly uses local memory in NUMA systems.
- **Data Affinity** - Similar to tasks affinity this is the concept of placing data in memory close to cores that access it the most. This is mainly a NUMA system problem because we want to avoid high remote memory access latency and improve spatial and temporal locality where a thread repeatedly works with the same data.
- **Memory Carried Dependency** - This is situation where one iteration in a loop depends on the data that was read from or written to in a different iteration of the loop.
- **Unrolled Loop** - An unrolled loop is where the loop body is repeated multiple times within a single iteration, reducing the number of operations (overhead) needed to manage the loop. So an unrolled loop can execute multiple iterations of a loop in a single iteration. This is normally by a factor $n$, i.e. $n$ iterations of work is unrolled into one iteration.
- **Canonical Form** - Modern x86-64 have the capacity for a full 64-bit address space but only really use the lower 48-bits (47-0) meaningfully whereas the upper 16-bits (63-48) are just  duplicates of the sign bit at bit 47. This allows CPUs to check if an address is valid very quickly by comparing the padding to the sign bit.
- **Out of Order Execution (OoO)** - This is a feature of modern CPUs where instructions can execute in a different order than the original program order, contingent on the final result being correct. For example, if one instruction is a `load` and the next is an `add` and they are independent from each other, you can execute the add while the load is occurring. Overall this allows for more effective CPU utilisation.
- **Reorder Buffer RoB** - a circular buffer that keeps track of destination register/memory, instruction type, program counter, status (instruction completed). This is what enables OoO execution safely without compromising correctness. When a RoB entry is complete it is **retired**. Retirement happens in **program order**. It's a buffer used to ensure final program order.
- **Read After Write (RAW)** - An instructions needs to read a value that a previous instruction writes. The OoO hardware must preserve order.
- **Write After Read (WAR)** - An instruction writes to a location after it has been read by a previous instruction. This can typically be resolved by simply renaming. If the write instruction executes before the read instruction, the read instruction is wrong. Renaming allows OoO hardware to perform its optimisations.
- **Write After Write** - Two instructions write to the same destination, order matters for correctness. Typically renaming or the RoB can be used to preserve program order.
- **Induction Variable** - A variable that changes systematically and predictably across iterations, it is important for compiler loop optimisations. For example, the loop counter is a very basic example, but can also include variables within the body.
- **Loop Invariant** - An expression where the reduction to a value is independent of the iteration variables. This is the basis for **loop invariant code motion** optimisation which moves this loop invariant outside the loop.
- **Peephole Optimisation** - This is a class of optimisation that only works based on a small window of context, or in other words locally.